# AI File Classifier Configuration

# Application Settings
app:
  name: "AI File Classifier"
  version: "1.0.0"
  log_level: "INFO" # DEBUG, INFO, WARNING, ERROR, CRITICAL
  log_file: "logs/classifier.log"
  log_rotation: true
  max_log_size_mb: 10
  # Cache Settings (OPTIMIZED)
  cache_enabled: true
  cache_dir: ".cache"
  cache_ttl_hours: 24
  cache_format: "binary"  # NEW: 'binary' (msgpack/pickle - 3-5x faster) or 'json' (compatible)
  # binary format uses msgpack if available, else pickle
  # ~3-5x faster serialization/deserialization than JSON

# API Configuration (OpenAI-compatible)
api:
  provider: "ollama" # openai, ollama, localai, custom
  api_key: "${OPENAI_API_KEY}" # Use environment variable
  base_url: "http://localhost:11434/v1"
  model_name: "gemma3:latest"
  temperature: 0.2
  max_tokens: 1000
  timeout: 60  # Increased for batch requests with multiple files
  max_retries: 3
  retry_delay: 2 # seconds
  max_concurrent_requests: 1  # Not used in sequential mode (kept for compatibility)
  requests_per_minute: 60

# Classification Settings
classification:
  default_strategy: "content_based"
  confidence_threshold: 0.5
  fallback_strategy: "heuristic"
  max_depth: 3

  strategies:
    content_based:
      enabled: true
      weight: 1.0

    project_based:
      enabled: true
      weight: 0.8
      project_indicators: ["README", "package.json", ".git"]

    date_based:
      enabled: false
      format: "YYYY/MM"

    type_based:
      enabled: true
      weight: 0.6

# File Scanning Settings
scanning:
  recursive: true
  follow_symlinks: false
  max_depth: null # null for unlimited
  ignore_hidden: true
  ignore_patterns:
    - "node_modules"
    - ".git"
    - "__pycache__"
    - "*.tmp"
    - ".DS_Store"

  file_filters:
    extensions:
      include: [] # Empty = all files
      exclude: [".exe", ".dll", ".so"]

    size:
      min_bytes: 0
      max_bytes: 104857600 # 100 MB

    date:
      modified_after: null
      modified_before: null

# Directory Management
directories:
  naming_convention: "snake_case" # snake_case, kebab-case, PascalCase, camelCase
  sanitize_names: true
  max_name_length: 100
  conflict_resolution: "append_counter" # append_counter, append_timestamp, skip
  create_index_files: false

# Language Settings
language:
  primary: "english" # primary language for directory names (use --language CLI arg to override)
  fallback: "english" # fallback if primary not available
  supported_languages:
    - "indonesian"
    - "english"
    - "spanish"
    - "french"
    - "german"
    - "japanese"
    - "chinese"
  # Language-specific naming examples:
  # Indonesian: dokumen_keuangan, laporan_tahunan
  # English: financial_documents, annual_reports

# File Operations
operations:
  mode: "move" # move or copy
  preserve_metadata: true
  preserve_permissions: true
  atomic_operations: true
  verify_after_move: true

  duplicate_handling: "rename" # skip, rename, overwrite
  rename_pattern: "{name}_{counter}{ext}"

  backup:
    enabled: false
    backup_dir: ".backup"
    keep_days: 7

# Performance Settings (OPTIMIZED)
performance:
  batch_size: 50  # Legacy setting, use batch_processing.batch_size instead
  max_workers: 10  # Parallel workers for file I/O operations (NEW: 10x faster file scanning)
  max_memory_mb: 500
  enable_profiling: false

  # Advanced Batch Processing Configuration
  batch_processing:
    # Enable/disable batch processing (if false, uses serial processing)
    enabled: true

    # Batch size: number of files to process per batch
    # Larger batches = better throughput, more memory usage
    # Recommended: 20-100 for local Ollama, 50-200 for cloud APIs
    batch_size: 50

    # File grouping strategy for intelligent batching (ENHANCED)
    # Options: 'extension' | 'size' | 'mixed' | 'semantic' | 'balanced' | 'none'
    # - extension: Groups files by type for better classification context
    # - size: Groups by size categories for memory management
    # - mixed: Hybrid approach balancing both factors (RECOMMENDED)
    # - semantic: Groups by file category (code, docs, media) - best accuracy
    # - balanced: Distributes file types evenly across batches - consistent performance
    # - none: No grouping (process in original order)
    grouping_strategy: 'semantic'  # CHANGED: semantic grouping for better accuracy

    # Multi-file batch requests - TRUE BATCH PROCESSING
    # Sends multiple files in a single LLM request (dramatically reduces API calls)
    # NOTE: Batches are processed SEQUENTIALLY (one at a time) to prevent overwhelming the LLM
    multi_file_requests:
      # Enable TRUE batch processing (multiple files per API request)
      # ENABLED BY DEFAULT for maximum performance and cost savings
      enabled: true

      # Maximum files per single API request
      # Higher = fewer API calls but larger token usage per request
      # Default: 10 files per request (100 files = 10 sequential API requests instead of 100!)
      # Recommended ranges:
      #   - Local Ollama: 5-10 files per request (processed sequentially)
      #   - Cloud APIs (OpenAI/Anthropic): 10-20 files per request
      #   - Small context models: 3-5 files per request
      # NOTE: Batches are processed one at a time, not concurrently
      max_files_per_request: 10

      # Only use multi-file batching for files with same extension
      # Set to false to batch any files together
      same_extension_only: false

    # Performance tuning
    tuning:
      # Adaptive batching: automatically adjust batch size based on performance
      adaptive_batch_size: false

      # Target processing rate (files per second)
      # System will try to optimize batch size to achieve this rate
      target_rate: 2.0

      # Memory-aware batching: reduce batch size if memory usage is high
      memory_aware: true
      memory_threshold_percent: 80

  # Content Analysis Settings (OPTIMIZED)
  content_analysis:
    max_file_size_mb: 1
    read_chunk_size_kb: 64
    max_content_length: 5000  # characters
    smart_sampling: true  # NEW: Intelligent sampling (beginning, middle, end) for large files
    # Smart sampling provides better file representation than just reading the beginning
    # Especially useful for large files where important context might be at the end
    # ~2-3x faster for large files while maintaining classification accuracy

# Reporting
reporting:
  enabled: true
  output_dir: "reports"
  formats: ["json", "csv", "html"]
  include_details: true
  include_statistics: true
  include_errors: true

# Extensions/Plugins
extensions:
  enabled: true
  plugin_directory: "./plugins"
  auto_load: true

# User Preferences
preferences:
  interactive_mode: false
  confirm_before_execute: true
  show_progress: true
  dry_run_default: false
  verbose: false
